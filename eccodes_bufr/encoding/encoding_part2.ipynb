{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1071ad-c303-4ff6-84ad-d555ed1d4e64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# <center>**Adding more data to the BUFR file** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a469f84-9ff9-4fe4-a495-feff26219ea1",
   "metadata": {},
   "source": [
    "Here,we will add more observations to the BUFR file. For example we may observations for different times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76153831-48d1-4dda-92b8-8b94d75b908b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id       ymd  hhmm  latitude  longitude  wheight  avg_period  pressure\n",
      "0  41049  20251001  1540     27.51     -62.27      6.6         9.9    1009.4\n",
      "1  41049  20251001  1600     27.54     -62.77      5.5         7.5    1012.2\n",
      "2  41049  20251001  1710     27.51     -62.77      5.2         7.8    1014.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fname=\"nassau3.csv\"\n",
    "df_obs=pd.read_csv(fname)\n",
    "print(df_obs.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "536cac4b-ce66-482b-ab2d-19ae5c904694",
   "metadata": {},
   "source": [
    "so we have now 3 more observations taken at different times from the same station\n",
    "Our sequence has to accommodate this. There are two ways of doing this\n",
    "1) using compressedData=1 ( compressed data,the values of each descriptor are grouped together across all subsets)\n",
    "2) using compressedData=0 ( individual subsets are stored individually and one after the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e254534-eb4d-47f0-8f02-3ae6d74192d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eccodes as ecc\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "711d9751-2a80-4142-a2f0-41585f9125a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id       ymd  hhmm  latitude  longitude  wheight  avg_period  pressure\n",
      "0  41049  20251001  1540     27.51     -62.27      6.6         9.9    1009.4\n",
      "1  41049  20251001  1600     27.54     -62.77      5.5         7.5    1012.2\n",
      "2  41049  20251001  1710     27.51     -62.77      5.2         7.8    1014.5\n",
      " created file /home/marg/ECCODES_2025/ecTrain/nassau3.b\n"
     ]
    }
   ],
   "source": [
    "def encode_bufr(df_obs,outputFilename):\n",
    "    '''\n",
    "    encodes df_obs into a BUFR file refereed by outputFilename, uses compressedData=1\n",
    "    '''\n",
    "    nobs=df_obs.index.size\n",
    "    bid=ecc.codes_bufr_new_from_samples('BUFR4') # creates a bufr edition 4 ( WMO recommended)\n",
    "    \n",
    "    \n",
    "    ecc.codes_set(bid,'masterTablesVersionNumber',42)\n",
    "    ecc.codes_set(bid,'localTablesVersionNumber',0)\n",
    "    \n",
    "    ecc.codes_set(bid,'numberOfSubsets',nobs)\n",
    "    # here, we read the YMD and HHMM to build the time stamp\n",
    "    # this timestamp is used to populate the BUFR time keys\n",
    "    ymd=df_obs['ymd'].values[0]\n",
    "    hhmm=df_obs['hhmm'].values[0] \n",
    "    str_ymdhm=str(ymd)+str(hhmm)\n",
    "    timeStamp=datetime.strptime(str_ymdhm,'%Y%m%d%H%M')\n",
    "    ecc.codes_set(bid,\"typicalYear\",timeStamp.year)\n",
    "    ecc.codes_set(bid,\"typicalMonth\",timeStamp.month)\n",
    "    ecc.codes_set(bid,\"typicalDay\",timeStamp.day)\n",
    "    ecc.codes_set(bid,\"typicalHour\",timeStamp.hour)\n",
    "    ecc.codes_set(bid,\"typicalMinute\",timeStamp.minute)\n",
    "    ecc.codes_set(bid,'numberOfSubsets',nobs)\n",
    "    ecc.codes_set(bid,\"observedData\",1)\n",
    "    ecc.codes_set(bid,'compressedData',1) # data is 'compressed'\n",
    "    # here we define the BUFR structure through the descriptors we want to use\n",
    "    unexpandedDescr=[ 1015,4001,4002,4003,4004,4005,\n",
    "                     5001,6001,22021,22011,10051]\n",
    "    ecc.codes_set_array(bid,'unexpandedDescriptors',unexpandedDescr)\n",
    "    station_id=np.array([str(x) for x in df_obs['id'].values])\n",
    "    ecc.codes_set_array(bid,'stationOrSiteName',station_id)\n",
    "\n",
    "    ymd_values  = df_obs['ymd'].values\n",
    "    hhmm_values = df_obs['hhmm'].values\n",
    "    dt_dates    = [datetime.strptime(str(x)+str(y),'%Y%m%d%H%M') for x,y in zip(ymd_values,hhmm_values)]\n",
    "\n",
    "    years      = [t.year for t in dt_dates]\n",
    "    months     = [t.month for t in dt_dates]\n",
    "    days       = [t.day for t in dt_dates]\n",
    "    hours      = [t.hour for t in dt_dates]\n",
    "    minutes    = [t.minute for t in dt_dates]\n",
    "    ecc.codes_set_array(bid,'year',years)\n",
    "    ecc.codes_set_array(bid,'month',months)\n",
    "    ecc.codes_set_array(bid,'day',days)\n",
    "    ecc.codes_set_array(bid,'hour',hours)\n",
    "    ecc.codes_set_array(bid,'minute',minutes)\n",
    "   \n",
    "    lat=df_obs['latitude'].values\n",
    "    lon=df_obs['longitude'].values\n",
    "    ecc.codes_set_array(bid,\"latitude\",lat)\n",
    "    ecc.codes_set_array(bid,\"longitude\",lon)\n",
    "    waveHeight=df_obs['wheight'].values\n",
    "    ecc.codes_set_array(bid,\"heightOfWaves\",waveHeight)\n",
    "    avg_period=df_obs['avg_period'].values\n",
    "    ecc.codes_set_array(bid,\"periodOfWaves\",avg_period)\n",
    "    pressure=df_obs['pressure'].values\n",
    "    ecc.codes_set_array(bid,'pressureReducedToMeanSeaLevel',pressure)\n",
    "    ecc.codes_set(bid,\"pack\",1)\n",
    "    with open(outputFilename,\"wb\") as fout:\n",
    "        ecc.codes_write(bid,fout)\n",
    "    ecc.codes_release(bid)\n",
    "    print(f\" created file {outputFilename}\")\n",
    "    return \n",
    "\n",
    "def main():\n",
    "    inputFile='/home/marg/ECCODES_2025/ecTrain/nassau3.csv'\n",
    "    outputFile='/home/marg/ECCODES_2025/ecTrain/nassau3.b'\n",
    "    df=pd.read_csv(inputFile)\n",
    "    print(df.head())\n",
    "    encode_bufr(df,outputFile)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b551f41-3779-41f9-92be-dafd6810fdfe",
   "metadata": {},
   "source": [
    "If we use compressedData=1 (compressed data) we then see arrays of data for each descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b788a-dbf8-4ea1-b432-16677e010f2e",
   "metadata": {},
   "source": [
    " {\n",
    "                        \"key\" : \"periodOfWaves\",\n",
    "                        \"value\" :\n",
    "                        [\n",
    "                          10, 8, 8 \n",
    "                        ],\n",
    "                        \"units\" : \"s\"\n",
    "                      },\n",
    "                      {\n",
    "                        \"key\" : \"pressureReducedToMeanSeaLevel\",\n",
    "                        \"value\" :\n",
    "                        [\n",
    "                          1010, 1010, 1010 \n",
    "                        ],\n",
    "                        \"units\" : \"Pa\"\n",
    "                      }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764b37d-d82c-4521-a898-a2e8e4d11c48",
   "metadata": {},
   "source": [
    "we can modify the code above to use compressedData=0 so the data will be spit in individual subsets each containing all the descriptors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0127f999-b80d-48b4-9588-267ad92fdebd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id       ymd  hhmm  latitude  longitude  wheight  avg_period  pressure\n",
      "0  41049  20251001  1540     27.51     -62.27      6.6         9.9    1009.4\n",
      "1  41049  20251001  1600     27.54     -62.77      5.5         7.5    1012.2\n",
      "2  41049  20251001  1710     27.51     -62.77      5.2         7.8    1014.5\n",
      " created file /home/marg/ECCODES_2025/ecTrain/nassau3_unc.b\n"
     ]
    }
   ],
   "source": [
    "def encode_bufr_uncompressed(df_obs,outputFilename):\n",
    "    '''\n",
    "    encodes df_obs into a bufr file referred by outputFilename uncompressed data\n",
    "    '''\n",
    "    nobs=df_obs.index.size\n",
    "    bid=ecc.codes_bufr_new_from_samples('BUFR4') # creates a bufr edition 4 ( WMO recommended)\n",
    "    \n",
    "    \n",
    "    ecc.codes_set(bid,'masterTablesVersionNumber',42)\n",
    "    ecc.codes_set(bid,'localTablesVersionNumber',0)\n",
    "    \n",
    "    ecc.codes_set(bid,'numberOfSubsets',nobs)\n",
    "    # here, we read the YMD and HHMM to build the time stamp\n",
    "    # this timestamp is used to populate the BUFR time keys\n",
    "    ymd=df_obs['ymd'].values[0]\n",
    "    hhmm=df_obs['hhmm'].values[0] \n",
    "    str_ymdhm=str(ymd)+str(hhmm)\n",
    "    timeStamp=datetime.strptime(str_ymdhm,'%Y%m%d%H%M')\n",
    "    ecc.codes_set(bid,\"typicalYear\",timeStamp.year)\n",
    "    ecc.codes_set(bid,\"typicalMonth\",timeStamp.month)\n",
    "    ecc.codes_set(bid,\"typicalDay\",timeStamp.day)\n",
    "    ecc.codes_set(bid,\"typicalHour\",timeStamp.hour)\n",
    "    ecc.codes_set(bid,\"typicalMinute\",timeStamp.minute)\n",
    "    ecc.codes_set(bid,'numberOfSubsets',nobs)\n",
    "    ecc.codes_set(bid,\"observedData\",1)\n",
    "    ecc.codes_set(bid,'compressedData',0) # data is  NOT 'compressed'\n",
    "    # here we define the BUFR structure through the descriptors we want to use\n",
    "    unexpandedDescr=[ 1015,4001,4002,4003,4004,4005,\n",
    "                     5001,6001,22021,22011,10051]\n",
    "    ecc.codes_set_array(bid,'unexpandedDescriptors',unexpandedDescr)\n",
    "    \n",
    "    for i,row in df_obs.iterrows():\n",
    "        st_id  = int(row['id'])\n",
    "        ecc.codes_set(bid,f'#{i+1}#stationOrSiteName',str(st_id))\n",
    "        ymd    = str(df_obs['ymd'].values[0])\n",
    "        hhmm   = str(df_obs['hhmm'].values[0])\n",
    "    \n",
    "        date   = datetime.strptime(ymd+hhmm,\"%Y%m%d%H%M\")\n",
    "\n",
    " \n",
    "        ecc.codes_set(bid,f'#{i+1}#year',date.year)\n",
    "        ecc.codes_set(bid,f'#{i+1}#month',date.month)\n",
    "        ecc.codes_set(bid,f'#{i+1}#day',date.day)\n",
    "        ecc.codes_set(bid,f'#{i+1}#hour',date.hour)\n",
    "        ecc.codes_set(bid,f'#{i+1}#minute',date.minute)\n",
    "   \n",
    "        lat=df_obs['latitude'].values[0]\n",
    "        lon=df_obs['longitude'].values[0]\n",
    "        ecc.codes_set(bid,f\"#{i+1}#latitude\",lat)\n",
    "        ecc.codes_set(bid,f\"#{i+1}#longitude\",lon)\n",
    "        waveHeight=df_obs['wheight'].values[0]\n",
    "        ecc.codes_set(bid,f\"#{i+1}#heightOfWaves\",waveHeight)\n",
    "        avg_period=df_obs['avg_period'].values[0]\n",
    "        ecc.codes_set(bid,f\"#{i+1}#periodOfWaves\",avg_period)\n",
    "        pressure=df_obs['pressure'].values[0]\n",
    "        ecc.codes_set(bid,f'#{i+1}#pressureReducedToMeanSeaLevel',pressure)\n",
    "    ecc.codes_set(bid,\"pack\",1)\n",
    "    with open(outputFilename,\"wb\") as fout:\n",
    "        ecc.codes_write(bid,fout)\n",
    "    ecc.codes_release(bid)\n",
    "    print(f\" created file {outputFilename}\")\n",
    "    return \n",
    "\n",
    "def main():\n",
    "    inputFile='/home/marg/ECCODES_2025/ecTrain/nassau3.csv'\n",
    "    outputFile='/home/marg/ECCODES_2025/ecTrain/nassau3_unc.b'\n",
    "    df=pd.read_csv(inputFile)\n",
    "    print(df.head())\n",
    "    encode_bufr_uncompressed(df,outputFile)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc154d6-e8e8-47ee-9da7-7d86fe856a7e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this case, as the data is uncompressed, we have it split into individual subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eedbfc-309d-4bb5-a972-cb943b090062",
   "metadata": {},
   "source": [
    "\n",
    "    {\n",
    "          \"key\" : \"subsetNumber\",\n",
    "          \"value\" : 1\n",
    "        },\n",
    "        {\n",
    "          \"key\" : \"stationOrSiteName\",\n",
    "          \"value\" : \"41049\",\n",
    "          \"units\" : \"CCITT IA5\"\n",
    "        },\n",
    "        [\n",
    "\n",
    "          {\n",
    "            \"key\" : \"year\",\n",
    "            \"value\" : 2025,\n",
    "            \"units\" : \"a\"\n",
    "          },\n",
    "          [\n",
    "\n",
    "            {\n",
    "              \"key\" : \"month\",\n",
    "              \"value\" : 10,\n",
    "              \"units\" : \"mon\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1466fe0-e29a-4c2a-b635-24439b5c7529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.9-01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
